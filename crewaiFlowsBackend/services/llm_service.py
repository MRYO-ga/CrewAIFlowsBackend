"""
LLMæœåŠ¡æ¨¡å—
è´Ÿè´£å¤„ç†å¤§è¯­è¨€æ¨¡å‹çš„è°ƒç”¨ã€ä»»åŠ¡æ‹†è§£å’Œå·¥å…·é›†æˆ
"""

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
from datetime import datetime
from enum import Enum

import sys
import os
from pathlib import Path

# æ·»åŠ utilsè·¯å¾„åˆ°sys.path
utils_path = Path(__file__).parent.parent / "utils"
if str(utils_path) not in sys.path:
    sys.path.append(str(utils_path))

from myLLM import chat_with_llm
from pydantic import BaseModel

from .tool_service import ToolService
from .mcp_client_service import MCPClientService, LogType

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    SIMPLE_QUERY = "simple_query"  # ç®€å•æŸ¥è¯¢ï¼Œæ— éœ€å·¥å…·
    DATA_READ = "data_read"  # æ•°æ®è¯»å–
    DATA_WRITE = "data_write"  # æ•°æ®å†™å…¥
    DATA_ANALYSIS = "data_analysis"  # æ•°æ®åˆ†æ
    COMPLEX_TASK = "complex_task"  # å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤

class TaskStep(BaseModel):
    """ä»»åŠ¡æ­¥éª¤æ¨¡å‹"""
    step_id: int
    step_name: str
    step_description: str
    tool_name: Optional[str] = None
    tool_args: Optional[Dict[str, Any]] = None
    result: Optional[Any] = None
    status: str = "pending"  # pending, running, completed, failed
    error: Optional[str] = None

class TaskDecomposition(BaseModel):
    """ä»»åŠ¡æ‹†è§£ç»“æœæ¨¡å‹"""
    task_type: TaskType
    task_description: str
    steps: List[TaskStep]
    estimated_time: Optional[int] = None  # é¢„ä¼°æ—¶é—´ï¼ˆç§’ï¼‰
    requires_tools: bool = False
    tool_names: List[str] = []

class LLMResponse(BaseModel):
    """LLMå“åº”æ¨¡å‹"""
    content: str
    task_decomposition: Optional[TaskDecomposition] = None
    tool_calls: List[Dict[str, Any]] = []
    steps_executed: List[TaskStep] = []
    final_answer: str
    metadata: Dict[str, Any] = {}

class LLMService:
    """LLMæœåŠ¡ç±»"""
    
    def __init__(self, tool_service: ToolService, api_key: str = None, model: str = "gpt-4o-mini", llm_type: str = "openai"):
        """
        åˆå§‹åŒ–LLMæœåŠ¡
        
        Args:
            tool_service: å·¥å…·æœåŠ¡å®ä¾‹
            api_key: APIå¯†é’¥ï¼ˆå·²é…ç½®åœ¨myLLM.pyä¸­ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            llm_type: LLMç±»å‹ï¼ˆopenai, oneapi, ollamaï¼‰
        """
        self.tool_service = tool_service
        self.model = model
        self.llm_type = llm_type
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ¤– åˆå§‹åŒ–LLMæœåŠ¡: ç±»å‹={llm_type}, æ¨¡å‹={model}")
        
        # ä¸å†ä½¿ç”¨AsyncOpenAIå®¢æˆ·ç«¯ï¼Œè€Œæ˜¯ä½¿ç”¨myLLMä¸­çš„é…ç½®
        
        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚
ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹èƒ½åŠ›ï¼š
1. åˆ†æç”¨æˆ·é—®é¢˜å¹¶æ‹†è§£ä¸ºå…·ä½“æ­¥éª¤
2. è°ƒç”¨å·¥å…·è·å–å’Œå¤„ç†æ•°æ®
3. æä¾›æ¸…æ™°çš„æ­¥éª¤æ‰§è¡Œè¿‡ç¨‹
4. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆå’Œå»ºè®®

å½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æµç¨‹ï¼š
1. åˆ†æé—®é¢˜ç±»å‹å’Œå¤æ‚åº¦
2. å¦‚æœéœ€è¦å·¥å…·ï¼Œæ‹†è§£ä¸ºå…·ä½“æ­¥éª¤
3. æŒ‰æ­¥éª¤æ‰§è¡Œå¹¶å±•ç¤ºè¿‡ç¨‹
4. æä¾›æœ€ç»ˆç­”æ¡ˆ

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
        
    async def analyze_user_input(self, user_input: str, conversation_history: Optional[List[Dict[str, Any]]] = None) -> TaskDecomposition:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¹¶æ‹†è§£ä»»åŠ¡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            ä»»åŠ¡æ‹†è§£ç»“æœ
        """
        try:
            print(f"ğŸ” [åˆ†æé˜¶æ®µ] å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥: '{user_input}'")
            
            # è·å–å¯ç”¨å·¥å…·
            print("ğŸ“¦ [åˆ†æé˜¶æ®µ] æ­£åœ¨è·å–å¯ç”¨å·¥å…·åˆ—è¡¨...")
            available_tools = await self.tool_service.get_tools_for_llm()
            print(f"âœ… [åˆ†æé˜¶æ®µ] è·å–åˆ° {len(available_tools)} ä¸ªå¯ç”¨å·¥å…·:")
            for i, tool in enumerate(available_tools, 1):
                print(f"   {i}. {tool['name']}: {tool['description']}")
            
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_input}

å¯ç”¨å·¥å…·ï¼š
{json.dumps(available_tools, ensure_ascii=False, indent=2)}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "task_type": "simple_query|data_read|data_write|data_analysis|complex_task",
    "task_description": "ä»»åŠ¡æè¿°",
    "requires_tools": true/false,
    "tool_names": ["å·¥å…·åç§°åˆ—è¡¨"],
    "steps": [
        {{
            "step_id": 1,
            "step_name": "æ­¥éª¤åç§°",
            "step_description": "æ­¥éª¤æè¿°",
            "tool_name": "å·¥å…·åç§°æˆ–null",
            "tool_args": {{"å‚æ•°": "å€¼"}}
        }}
    ],
    "estimated_time": é¢„ä¼°æ—¶é—´ç§’æ•°
}}

è¯·åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            # è°ƒç”¨LLMåˆ†æ
            print("ğŸ§  [åˆ†æé˜¶æ®µ] å‡†å¤‡è°ƒç”¨LLMè¿›è¡Œä»»åŠ¡åˆ†æ...")
            print(f"ğŸ“ [åˆ†æé˜¶æ®µ] åˆ†ææç¤ºè¯é•¿åº¦: {len(analysis_prompt)} å­—ç¬¦")
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·é—®é¢˜æ‹†è§£ä¸ºå…·ä½“çš„æ‰§è¡Œæ­¥éª¤ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ]
            
            print("â³ [åˆ†æé˜¶æ®µ] æ­£åœ¨è°ƒç”¨LLM...")
            response = await self._call_llm(messages)
            print(f"ğŸ“„ [åˆ†æé˜¶æ®µ] LLMåˆ†æå“åº”: {response[:200]}..." if len(response) > 200 else f"ğŸ“„ [åˆ†æé˜¶æ®µ] LLMåˆ†æå“åº”: {response}")
            
            # è§£æè¿”å›ç»“æœ
            print("ğŸ”§ [åˆ†æé˜¶æ®µ] å¼€å§‹è§£æLLMå“åº”...")
            try:
                analysis_result = json.loads(response)
                print("âœ… [åˆ†æé˜¶æ®µ] JSONè§£ææˆåŠŸ")
                print(f"ğŸ“‹ [åˆ†æé˜¶æ®µ] ä»»åŠ¡ç±»å‹: {analysis_result.get('task_type', 'unknown')}")
                print(f"ğŸ¯ [åˆ†æé˜¶æ®µ] ä»»åŠ¡æè¿°: {analysis_result.get('task_description', 'unknown')}")
                print(f"ğŸ› ï¸ [åˆ†æé˜¶æ®µ] éœ€è¦å·¥å…·: {analysis_result.get('requires_tools', False)}")
                print(f"ğŸ“ [åˆ†æé˜¶æ®µ] å·¥å…·åˆ—è¡¨: {analysis_result.get('tool_names', [])}")
                print(f"â±ï¸ [åˆ†æé˜¶æ®µ] é¢„è®¡æ—¶é—´: {analysis_result.get('estimated_time', 'unknown')} ç§’")
                
                # æ„å»ºTaskStepå¯¹è±¡
                steps = []
                steps_data = analysis_result.get("steps", [])
                print(f"ğŸ“Š [åˆ†æé˜¶æ®µ] æ‹†è§£ä¸º {len(steps_data)} ä¸ªæ­¥éª¤:")
                
                for step_data in steps_data:
                    step = TaskStep(
                        step_id=step_data.get("step_id", 0),
                        step_name=step_data.get("step_name", ""),
                        step_description=step_data.get("step_description", ""),
                        tool_name=step_data.get("tool_name"),
                        tool_args=step_data.get("tool_args")
                    )
                    steps.append(step)
                    print(f"   æ­¥éª¤{step.step_id}: {step.step_name} (å·¥å…·: {step.tool_name or 'æ— '})")
                
                # æ„å»ºTaskDecompositionå¯¹è±¡
                task_decomposition = TaskDecomposition(
                    task_type=TaskType(analysis_result.get("task_type", "simple_query")),
                    task_description=analysis_result.get("task_description", user_input),
                    steps=steps,
                    estimated_time=analysis_result.get("estimated_time"),
                    requires_tools=analysis_result.get("requires_tools", False),
                    tool_names=analysis_result.get("tool_names", [])
                )
                
                print("ğŸ‰ [åˆ†æé˜¶æ®µ] ä»»åŠ¡åˆ†æå®Œæˆï¼")
                return task_decomposition
                
            except json.JSONDecodeError as e:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºç®€å•ä»»åŠ¡
                print(f"âŒ [åˆ†æé˜¶æ®µ] JSONè§£æå¤±è´¥: {e}")
                print(f"ğŸ“„ [åˆ†æé˜¶æ®µ] åŸå§‹å“åº”: {response}")
                print("ğŸ”„ [åˆ†æé˜¶æ®µ] åˆ›å»ºé»˜è®¤ç®€å•ä»»åŠ¡...")
                self.logger.warning("LLMè¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ›å»ºç®€å•ä»»åŠ¡")
                return TaskDecomposition(
                    task_type=TaskType.SIMPLE_QUERY,
                    task_description=user_input,
                    steps=[TaskStep(
                        step_id=1,
                        step_name="å›ç­”é—®é¢˜",
                        step_description="ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜",
                        tool_name=None,
                        tool_args=None
                    )],
                    requires_tools=False,
                    tool_names=[]
                )
                
        except Exception as error:
            self.logger.error(f"åˆ†æç”¨æˆ·è¾“å…¥å¤±è´¥: {error}")
            # è¿”å›é»˜è®¤çš„ç®€å•ä»»åŠ¡
            return TaskDecomposition(
                task_type=TaskType.SIMPLE_QUERY,
                task_description=user_input,
                steps=[TaskStep(
                    step_id=1,
                    step_name="å›ç­”é—®é¢˜",
                    step_description="ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜",
                    tool_name=None,
                    tool_args=None
                )],
                requires_tools=False,
                tool_names=[]
            )
    
    async def execute_task(self, task_decomposition: TaskDecomposition, user_input: str) -> LLMResponse:
        """
        æ‰§è¡Œä»»åŠ¡
        
        Args:
            task_decomposition: ä»»åŠ¡æ‹†è§£ç»“æœ
            user_input: åŸå§‹ç”¨æˆ·è¾“å…¥
            
        Returns:
            LLMå“åº”ç»“æœ
        """
        try:
            print(f"ğŸš€ [æ‰§è¡Œé˜¶æ®µ] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_decomposition.task_description}")
            print(f"ğŸ“‹ [æ‰§è¡Œé˜¶æ®µ] ä»»åŠ¡ç±»å‹: {task_decomposition.task_type.value}")
            print(f"ğŸ› ï¸ [æ‰§è¡Œé˜¶æ®µ] éœ€è¦å·¥å…·: {task_decomposition.requires_tools}")
            print(f"ğŸ“Š [æ‰§è¡Œé˜¶æ®µ] å…±æœ‰ {len(task_decomposition.steps)} ä¸ªæ­¥éª¤éœ€è¦æ‰§è¡Œ")
            
            executed_steps = []
            tool_results = []
            
            # æ‰§è¡Œæ¯ä¸ªæ­¥éª¤
            for step in task_decomposition.steps:
                print(f"\nâš¡ [æ‰§è¡Œé˜¶æ®µ] æ‰§è¡Œæ­¥éª¤ {step.step_id}: {step.step_name}")
                print(f"ğŸ“„ [æ‰§è¡Œé˜¶æ®µ] æ­¥éª¤æè¿°: {step.step_description}")
                print(f"ğŸ”§ [æ‰§è¡Œé˜¶æ®µ] éœ€è¦å·¥å…·: {step.tool_name or 'æ— '}")
                if step.tool_args:
                    print(f"ğŸ“ [æ‰§è¡Œé˜¶æ®µ] å·¥å…·å‚æ•°: {step.tool_args}")
                
                step.status = "running"
                self.logger.info(f"æ‰§è¡Œæ­¥éª¤ {step.step_id}: {step.step_name}")
                
                try:
                    if step.tool_name:
                        # éœ€è¦è°ƒç”¨å·¥å…·
                        print(f"ğŸ› ï¸ [æ‰§è¡Œé˜¶æ®µ] æ­£åœ¨è°ƒç”¨å·¥å…·: {step.tool_name}")
                        result = await self.tool_service.call_tool(step.tool_name, step.tool_args or {})
                        print(f"ğŸ“¦ [æ‰§è¡Œé˜¶æ®µ] å·¥å…·è°ƒç”¨è¿”å›åŸå§‹ç»“æœ: {type(result)}")
                        
                        step.result = self.tool_service.format_tool_result(result)
                        print(f"âœ… [æ‰§è¡Œé˜¶æ®µ] å·¥å…·ç»“æœæ ¼å¼åŒ–å®Œæˆ: {step.result}")
                        
                        tool_results.append({
                            "step_id": step.step_id,
                            "tool_name": step.tool_name,
                            "result": step.result
                        })
                    else:
                        # ä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥æ ‡è®°å®Œæˆ
                        print("â­ï¸ [æ‰§è¡Œé˜¶æ®µ] æ­¤æ­¥éª¤ä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥å®Œæˆ")
                        step.result = {"message": "æ­¥éª¤å®Œæˆ"}
                    
                    step.status = "completed"
                    print(f"âœ… [æ‰§è¡Œé˜¶æ®µ] æ­¥éª¤ {step.step_id} æ‰§è¡ŒæˆåŠŸ")
                    
                except Exception as error:
                    step.status = "failed"
                    step.error = str(error)
                    print(f"âŒ [æ‰§è¡Œé˜¶æ®µ] æ­¥éª¤ {step.step_id} æ‰§è¡Œå¤±è´¥: {error}")
                    self.logger.error(f"æ­¥éª¤ {step.step_id} æ‰§è¡Œå¤±è´¥: {error}")
                
                executed_steps.append(step)
            
            # ç”Ÿæˆæœ€ç»ˆå›ç­”
            final_answer = await self._generate_final_answer(user_input, executed_steps, tool_results)
            
            # æ„å»ºå“åº”
            response = LLMResponse(
                content=final_answer,
                task_decomposition=task_decomposition,
                steps_executed=executed_steps,
                final_answer=final_answer,
                metadata={
                    "execution_time": datetime.now().isoformat(),
                    "steps_count": len(executed_steps),
                    "tools_used": [step.tool_name for step in executed_steps if step.tool_name],
                    "success_rate": len([s for s in executed_steps if s.status == "completed"]) / len(executed_steps) if executed_steps else 0
                }
            )
            
            return response
            
        except Exception as error:
            self.logger.error(f"æ‰§è¡Œä»»åŠ¡å¤±è´¥: {error}")
            # è¿”å›é”™è¯¯å“åº”
            return LLMResponse(
                content=f"æ‰§è¡Œä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {error}",
                final_answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†é”™è¯¯: {error}",
                metadata={"error": str(error)}
            )
    
    async def _generate_final_answer(self, user_input: str, executed_steps: List[TaskStep], tool_results: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            executed_steps: æ‰§è¡Œçš„æ­¥éª¤
            tool_results: å·¥å…·è°ƒç”¨ç»“æœ
            
        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        try:
            print(f"\nğŸ“ [ç­”æ¡ˆç”Ÿæˆ] å¼€å§‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            print(f"â“ [ç­”æ¡ˆç”Ÿæˆ] ç”¨æˆ·é—®é¢˜: {user_input}")
            print(f"ğŸ“Š [ç­”æ¡ˆç”Ÿæˆ] å·²æ‰§è¡Œæ­¥éª¤æ•°: {len(executed_steps)}")
            print(f"ğŸ› ï¸ [ç­”æ¡ˆç”Ÿæˆ] å·¥å…·è°ƒç”¨ç»“æœæ•°: {len(tool_results)}")
            
            # ç»Ÿè®¡æ­¥éª¤çŠ¶æ€
            completed_steps = [s for s in executed_steps if s.status == "completed"]
            failed_steps = [s for s in executed_steps if s.status == "failed"]
            print(f"âœ… [ç­”æ¡ˆç”Ÿæˆ] æˆåŠŸæ­¥éª¤: {len(completed_steps)}")
            print(f"âŒ [ç­”æ¡ˆç”Ÿæˆ] å¤±è´¥æ­¥éª¤: {len(failed_steps)}")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            print("ğŸ”§ [ç­”æ¡ˆç”Ÿæˆ] æ­£åœ¨æ„å»ºä¸Šä¸‹æ–‡...")
            context = f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_input}

æ‰§è¡Œçš„æ­¥éª¤ï¼š
"""
            for step in executed_steps:
                context += f"\n{step.step_id}. {step.step_name} - {step.step_description}"
                if step.status == "completed" and step.result:
                    context += f"\n   ç»“æœï¼š{json.dumps(step.result, ensure_ascii=False, indent=2)}"
                elif step.status == "failed":
                    context += f"\n   é”™è¯¯ï¼š{step.error}"
                    
            print(f"ğŸ“„ [ç­”æ¡ˆç”Ÿæˆ] ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # ç”Ÿæˆå›ç­”æç¤º
            answer_prompt = f"""
åŸºäºä»¥ä¸Šæ‰§è¡Œçš„æ­¥éª¤å’Œç»“æœï¼Œè¯·ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªæ¸…æ™°ã€è¯¦ç»†çš„æœ€ç»ˆç­”æ¡ˆã€‚

è¦æ±‚ï¼š
1. ç”¨ä¸­æ–‡å›ç­”
2. æ€»ç»“æ‰§è¡Œçš„æ­¥éª¤
3. æä¾›å…·ä½“çš„ç»“æœæˆ–å»ºè®®
4. å¦‚æœæœ‰æ•°æ®ï¼Œè¯·ç”¨æ˜“æ‡‚çš„æ–¹å¼å‘ˆç°
5. å¦‚æœæœ‰é”™è¯¯ï¼Œè¯·è¯´æ˜å¹¶æä¾›è§£å†³å»ºè®®

è¯·ç›´æ¥æä¾›ç­”æ¡ˆï¼Œä¸è¦é‡å¤é—®é¢˜ã€‚
"""
            
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": context + "\n\n" + answer_prompt}
            ]
            
            print("ğŸ§  [ç­”æ¡ˆç”Ÿæˆ] æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
            print(f"ğŸ“ [ç­”æ¡ˆç”Ÿæˆ] æ¶ˆæ¯æ•°é‡: {len(messages)}")
            print(f"ğŸ“„ [ç­”æ¡ˆç”Ÿæˆ] æ€»è¾“å…¥é•¿åº¦: {len(messages[0]['content']) + len(messages[1]['content'])} å­—ç¬¦")
            
            final_answer = await self._call_llm(messages)
            print(f"âœ… [ç­”æ¡ˆç”Ÿæˆ] æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(final_answer)} å­—ç¬¦")
            print(f"ğŸ“‹ [ç­”æ¡ˆç”Ÿæˆ] ç­”æ¡ˆé¢„è§ˆ: {final_answer[:100]}..." if len(final_answer) > 100 else f"ğŸ“‹ [ç­”æ¡ˆç”Ÿæˆ] å®Œæ•´ç­”æ¡ˆ: {final_answer}")
            return final_answer
            
        except Exception as error:
            self.logger.error(f"ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå¤±è´¥: {error}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {error}"
    
    async def _call_llm(self, messages: List[Dict[str, str]]) -> str:
        """
        è°ƒç”¨LLM API
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            LLMå“åº”å†…å®¹
        """
        try:
            # è®°å½•LLMè¯·æ±‚
            self.tool_service.mcp_client.add_logs(
                {"messages": messages, "model": self.model, "llm_type": self.llm_type},
                LogType.LLM_REQUEST
            )
            
            print(f"ğŸ¤– è°ƒç”¨LLM: {self.llm_type}/{self.model}")
            
            # ä½¿ç”¨myLLM.pyä¸­çš„chat_with_llmå‡½æ•°
            response = chat_with_llm(messages, llmType=self.llm_type, model=self.model)
            
            # chat_with_llmè¿”å›çš„æ˜¯JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
            import json
            try:
                response_data = json.loads(response)
                content = response_data.get("reply", response)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨å“åº”å†…å®¹
                content = response
            
            print(f"âœ… LLMå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è®°å½•LLMå“åº”
            self.tool_service.mcp_client.add_logs(
                {"response": content[:500] + "..." if len(content) > 500 else content, "full_response": response},
                LogType.LLM_RESPONSE
            )
            
            return content
            
        except Exception as error:
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {error}"
            print(f"âŒ {error_msg}")
            self.tool_service.mcp_client.add_logs(
                {"error": str(error)},
                LogType.LLM_ERROR
            )
            raise error
    
    async def process_user_input(self, user_input: str, conversation_history: Optional[List[Dict[str, Any]]] = None) -> LLMResponse:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥çš„å®Œæ•´æµç¨‹
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            å®Œæ•´çš„LLMå“åº”
        """
        try:
            # 1. åˆ†æç”¨æˆ·è¾“å…¥
            task_decomposition = await self.analyze_user_input(user_input, conversation_history)
            
            # 2. æ‰§è¡Œä»»åŠ¡
            response = await self.execute_task(task_decomposition, user_input)
            
            return response
            
        except Exception as error:
            self.logger.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥å¤±è´¥: {error}")
            return LLMResponse(
                content=f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {error}",
                final_answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†é”™è¯¯: {error}",
                metadata={"error": str(error)}
            )
    
    async def stream_response(self, user_input: str, conversation_history: Optional[List[Dict[str, Any]]] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å“åº”ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            
        Yields:
            æµå¼å“åº”æ•°æ®
        """
        try:
            # å‘é€å¼€å§‹ä¿¡å·
            yield {"type": "start", "message": "å¼€å§‹å¤„ç†æ‚¨çš„è¯·æ±‚..."}
            
            # 1. åˆ†æç”¨æˆ·è¾“å…¥
            yield {"type": "analysis", "message": "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."}
            task_decomposition = await self.analyze_user_input(user_input, conversation_history)
            
            yield {
                "type": "decomposition",
                "message": f"å·²æ‹†è§£ä¸º {len(task_decomposition.steps)} ä¸ªæ­¥éª¤",
                "data": task_decomposition.dict()
            }
            
            # 2. æ‰§è¡Œæ­¥éª¤
            executed_steps = []
            tool_results = []
            
            for step in task_decomposition.steps:
                yield {
                    "type": "step_start",
                    "message": f"æ‰§è¡Œæ­¥éª¤ {step.step_id}: {step.step_name}",
                    "step": step.dict()
                }
                
                step.status = "running"
                
                try:
                    if step.tool_name:
                        result = await self.tool_service.call_tool(step.tool_name, step.tool_args or {})
                        step.result = self.tool_service.format_tool_result(result)
                        tool_results.append({
                            "step_id": step.step_id,
                            "tool_name": step.tool_name,
                            "result": step.result
                        })
                    else:
                        step.result = {"message": "æ­¥éª¤å®Œæˆ"}
                    
                    step.status = "completed"
                    
                    yield {
                        "type": "step_completed",
                        "message": f"æ­¥éª¤ {step.step_id} å®Œæˆ",
                        "step": step.dict()
                    }
                    
                except Exception as error:
                    step.status = "failed"
                    step.error = str(error)
                    
                    yield {
                        "type": "step_failed",
                        "message": f"æ­¥éª¤ {step.step_id} å¤±è´¥: {error}",
                        "step": step.dict()
                    }
                
                executed_steps.append(step)
            
            # 3. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            yield {"type": "generating", "message": "æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ..."}
            final_answer = await self._generate_final_answer(user_input, executed_steps, tool_results)
            
            # 4. å‘é€å®Œæˆä¿¡å·
            yield {
                "type": "completed",
                "message": "å¤„ç†å®Œæˆ",
                "final_answer": final_answer,
                "metadata": {
                    "steps_count": len(executed_steps),
                    "tools_used": [step.tool_name for step in executed_steps if step.tool_name],
                    "success_rate": len([s for s in executed_steps if s.status == "completed"]) / len(executed_steps) if executed_steps else 0
                }
            }
            
        except Exception as error:
            yield {
                "type": "error",
                "message": f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error}",
                "error": str(error)
            } 